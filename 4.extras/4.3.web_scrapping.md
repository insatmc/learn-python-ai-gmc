There is more information on the Internet than any human can absorb in a lifetime. What you need is not access to that information, but a scalable way to collect, organize, and analyze it.

# BeautifulSoup4
## import urlopen & BS4
```python
from urllib.request import urlopen
from urllib.request import Request
from bs4 import BeautifulSoup
```

## Open page
```python
quote_page = "http://hackernoon.com/how-to-train-your-robot-ai-for-everyone-69b96ad943e5"
req = Request(quote_page, headers={'User-Agent' : "Magic Browser"})
page = urlopen(req)
```

## Parse page
```python
soup = BeautifulSoup(page, "html.parser")
```

## Getting Values from the page
```python
name_box = soup.find("h1", attrs={"class": "graf--title"})
read_time = soup.find("span", attrs={"class": "readingTime"})

name = name_box.text.strip() # strip() is used to remove starting and trailing
print(name)

print(read_time.attrs["title"])
```
