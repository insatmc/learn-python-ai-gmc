// slide 1

# AI
We -human- have for thousands of years tried to understand how we think.
AI goes with idea one step furthor, with AI we want to create intelegence.


## The Turing Test approach
The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
operational definition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
or from a computer.

<img src="https://upload.wikimedia.org/wikipedia/commons/5/55/Turing_test_diagram.png">

## Problems that we want to solve with AI
The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.
- Reasoning, problem solving
- Knowledge representation
- Planning
- Learning
- Natural language processing
- Creativity
- General intelligence


// slide 2

## Agent who reacts to its environment

An agent is anything that can be viewed as perceiving its environment through sensors and
acting upon that environment through actuators.

### Representation of an agent
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/IntelligentAgent-SimpleReflex.png/408px-IntelligentAgent-SimpleReflex.png">

Analyse the case of the Boston Dynamics robot "Atlas":
<iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

###  Simple reflex agents
```ruby
function SIMPLE_REFLEX_AGENT(percept) returns an action
  persistent: rules, a set of condition–action rules

  state ← INTERPRET_INPUT(percept)
  rule ← RULE_MATCH(state, rules)
  action ← rule.ACTION
  return action
```

### Example of Rules: A vacuum-cleaner world with just two locations
<img src="https://3.bp.blogspot.com/-awZN_dv2vLI/VsVcMux__HI/AAAAAAAAAF0/-1ZRtzWXu9A/s1600/fig03.02.gif">

```javascript
I am in A and A Dirty => Clean it
I am in A and A Clean => Go Right
I am in B and B Dirty => Clean it
I am in A and B clean => Go Left
```


###  Goal-based agents
Knowing something about the current state of the environment is not always enough to decide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. The correct decision depends on where the taxi is trying to get to. In other words, as well
GOAL as a current state description, the agent needs some sort of goal information that describes
situations that are desirable—for example, being at the passenger’s destination. The agent
program can combine this with the model (the same information as was used in the modelbased
reflex agent) to choose actions that achieve the goal. Figure 2.13 shows the goal-based
agent’s structure.


// slide 3
## Solving problem by searching
### Modeling

A problem can be defined formally by five components:
- The <b>initial state</b> that the agent starts in.

- A description of the <b>possible actions</b> available to the agent. Given a particular state s,
ACTIONS(s) returns the set of actions that can be executed in s. We say that each of
APPLICABLE these actions is applicable in s.

- A description of what each action does; the formal name for this is the transition
model.

- The goal test, which determines whether a given state is a goal state.

- A path cost function that assigns a numeric cost to each path. The problem-solving
agent chooses a cost function that reflects its own performance measure.

### Example: The vacuum world
<img src="http://centurion2.com/AIHomework/Searching/VacuumWorld.JPG">

- States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations, each of which might or might not contain dirt.
- Initial state: Any state can be designated as the initial state
- Actions: In this simple environment, each state has just three actions: Left, Right, and
Suck.
- Transition model: The actions have their expected effects, except that moving Left in
the leftmost square, moving Right in the rightmost square, and Sucking in a clean square
have no effect.
- Goal test: This checks whether all the squares are clean.
- Path cost: Each step costs 1, so the path cost is the number of steps in the path.


### Solving
We will use example of solving the problem of going from place Arad to place Bucharest:

<img src="http://www.massey.ac.nz/~mjjohnso/notes/59302/fig04.02.gif" />

We start from the root element "Arad" then we expand our state with the possible actions: "Sibiu", "Timisoara" and "Zerind"

Each new node is called a "leaf" we test now if we get to our goal, if not we expand the state more and more until we get to our goal.

Another point we need to be careful of redundant paths.

<img src="http://centurion2.com/AIHomework/Searching/AradBucharestPartial.JPG" />


### Greedy best-first tree search
Select leaf with best cost to get to our goal

<img src="http://cs-alb-pc3.massey.ac.nz/notes/59302/fig04.03.gif" />


### A* search: Minimizing the total estimated solution cost

A* evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost
to get from the node to the goal:

<img src="http://cs-alb-pc3.massey.ac.nz/notes/59302/fig04.04.gif" />


# Let's code:
Write in python a function that use A* search to find a path from city 1 to city 2.
